{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from data_process import get_CIFAR10_data\n",
    "import math\n",
    "from scipy.spatial import distance\n",
    "#from models import KNN, Perceptron, SVM, Softmax\n",
    "from kaggle_submission import output_submission_csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we determine the number of images for each split and load the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change these numbers for experimentation\n",
    "# For submission we will use the default values \n",
    "TRAIN_IMAGES = 49000\n",
    "VAL_IMAGES = 1000\n",
    "TEST_IMAGES = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_CIFAR10_data(TRAIN_IMAGES, VAL_IMAGES, TEST_IMAGES)\n",
    "X_train, y_train = data['X_train'], data['y_train']\n",
    "X_val, y_val = data['X_val'], data['y_val']\n",
    "X_test, y_test = data['X_test'], data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sets of images from dimensions of **(N, 3, 32, 32) -> (N, 3072)** where N is the number of images so that each **3x32x32** image is represented by a single vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 3072) (49000,) (1000, 3072) (1000,) (5000, 3072) (5000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "if False:\n",
    "    X_train = X_train / 135\n",
    "\n",
    "    X_test = X_test / 135\n",
    "\n",
    "    X_val = X_val / 135\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes how well your model performs using accuracy as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(pred, y_test):\n",
    "    return np.sum(y_test==pred)/len(y_test)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class SVM():\n",
    "    def __init__(self,alpha=.01, epochs=100, learning_rate = .001, reg_const= .001, minibatch_sz = 256):\n",
    "        \"\"\"\n",
    "        Initialises Softmax classifier with initializing \n",
    "        weights, alpha(learning rate), number of epochs\n",
    "        and regularization constant.\n",
    "        \"\"\"\n",
    "        self.w = None\n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_const = reg_const\n",
    "        ## adding\n",
    "        np.random.seed(90)\n",
    "        self.minibatch_sz = 256\n",
    "        self.debug=True\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    https://stats.stackexchange.com/questions/155088/gradient-for-hinge-loss-multiclass\n",
    "    http://cs231n.github.io/optimization-1/#analytic\n",
    "    \"\"\"\n",
    "    def calc_gradient(self, X: np.ndarray, y:np.ndarray, epoch=0) -> ( np.ndarray, float ): \n",
    "        \n",
    "        \"\"\"\n",
    "            24:48 of video https://www.youtube.com/watch?v=h7iBpEHGVNc&t=2330s\n",
    "            csc231 2017 lecture 3        \n",
    "            Calculate gradient of the svm hinge loss.\n",
    "\n",
    "            Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "            of N examples.\n",
    "\n",
    "            Inputs:\n",
    "            - X_train: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "            - y_train: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "            that X[i] has label c, where 0 <= c < C.\n",
    "\n",
    "            Returns:\n",
    "            - gradient with respect to weights W; an array of same shape as W\n",
    "        \"\"\"\n",
    "        this_loss = 0.0\n",
    "        grad_w = np.zeros(self.w.shape) # initialize the gradient as zero\n",
    "        assert(grad_w.shape == self.w.shape)\n",
    "        N = X.shape[0]\n",
    "        \n",
    "        #regularization = self.l2_regularization_euclidean(self.w ,reg_const=self.reg_const)\n",
    "   \n",
    "        ### <!---- vectorized hinge loss\n",
    "    \n",
    "        ## calculate data loss\n",
    "        a_scores = X.dot(self.w.T) #[a]\n",
    "        a_s_y_i = np.reshape(a_scores[0][y],( a_scores[0][y].size, 1))\n",
    "        a_margins = np.maximum(0,a_scores - a_s_y_i + .87)  ## calculate the pairwise margins in scores\n",
    "        a_margins[y] = 0  ## zero out the margins for the correct classes\n",
    "        a_data_loss = np.sum(a_margins) /  a_margins.shape[0] ## normalize by counts of training size (batch)\n",
    "        if epoch % 50 == 0 : print(\"my\", a_data_loss)\n",
    "        ## calculate regularization cost\n",
    "        a_l2_regularization_loss =  self.l2_regularization_euclidean(self.w, self.reg_const)\n",
    "        a_loss_i = a_data_loss + a_l2_regularization_loss\n",
    "   \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        ### --->\n",
    "        \n",
    "        scores = X.dot(self.w.T) ##[a]\n",
    "        \n",
    "        true_class_scores = scores[np.arange(scores.shape[0]),y] # \n",
    "        # calculate pairwose margins\n",
    "        margins = np.maximum(0, scores - np.transpose(np.matrix(true_class_scores)) + 1)  #\n",
    "        # set the true class margin to zero\n",
    "        margins[np.arange(N),y] = 0 #\n",
    "        \n",
    "        #if self.debug: print('margins/grad_w', margins.shape, grad_w.shape)\n",
    "        \n",
    "        \n",
    "        this_loss = np.mean(np.sum(margins, axis=1))  ##calculate data loss\n",
    "        this_loss += 1 * self.reg_const * np.linalg.norm(self.w)**2 ##np.sum(self.w*self.w)**2\n",
    "        \n",
    "        if self.debug and epoch % 50 == 0: print('loss at {}th'.format(epoch+1), this_loss)\n",
    "        \n",
    "        #print(\"reg \",  0.5 * self.l2_regularization_euclidean(self.w ,reg_const=self.reg_const))\n",
    "\n",
    "        \n",
    "        ##margins is used to seed the gradient calculation\n",
    "        binarized = margins\n",
    "        assert(binarized.shape==margins.shape)\n",
    "        ## the derivative of the hinge function is the indicator function\n",
    "        ## called on each margin calculation - where a non-zero value results in a 1 \n",
    "        ## a 1 indicates a violation of the true class and incurs a penalty\n",
    "        binarized[margins > 0 ] = 1\n",
    "        ## binarized sum  \n",
    "        row_sum = np.sum(binarized, axis=1)\n",
    "        #if self.debug: print(\"row_sum\", row_sum.shape)\n",
    "        assert(grad_w.shape == self.w.shape)\n",
    "        binarized[np.arange(N), y] = - np.transpose(row_sum)\n",
    "        \n",
    "        assert(binarized.shape == margins.shape)\n",
    "        \n",
    "        grad_w = np.dot(np.transpose(binarized), X)\n",
    "        assert(grad_w.shape == self.w.shape)\n",
    "        \n",
    "        # average gradient\n",
    "        grad_w = grad_w / N\n",
    "        assert(grad_w.shape == self.w.shape)\n",
    "\n",
    "        # apply regularization to gradient vector\n",
    "        \n",
    "        \n",
    "        assert(grad_w.shape == self.w.shape)\n",
    "        \n",
    "        grad_w += self.alpha*self.w\n",
    "        \n",
    "        \n",
    "        assert(grad_w.shape == self.w.shape)\n",
    "        return grad_w, this_loss\n",
    "    ########  \n",
    "\n",
    "        \n",
    "    def train(self, X_train : np.ndarray, y_train : np.ndarray):\n",
    "        if self.debug: print(np.min(X_train[0]), np.max(X_train[0]), np.mean(X_train[0]))\n",
    "        if self.debug: print(\"y_train\", y_train.shape)\n",
    "        current_epoch = 0\n",
    "        \"\"\"\n",
    "        Train SVM classifier using stochastic gradient descent.\n",
    "\n",
    "        Inputs:\n",
    "        - X_train: A numpy array of shape (N, D) containing training data;\n",
    "        N examples with D dimensions\n",
    "        - y_train: A numpy array of shape (N,) containing training labels;\n",
    "        \n",
    "        Hint : Operate with Minibatches of the data for SGD\n",
    "        \"\"\"\n",
    "        # initialize weight\n",
    "        self._initialize_weights(X_train, y_train)\n",
    "        loss_i = 99999999\n",
    "        \n",
    "        for X_batch, y_batch in self._minibatches(X_train, y_train, self.minibatch_sz): # [1]\n",
    "            gradient_w, this_loss = self.calc_gradient( X_batch, y_batch, epoch=current_epoch) #[2]\n",
    "            if abs(this_loss - loss_i) < .00001:\n",
    "                break\n",
    "            loss_i = this_loss\n",
    "            # update  [3]\n",
    "            \n",
    "            self.w += gradient_w * self.learning_rate\n",
    "            \n",
    "            current_epoch += 1\n",
    "            if current_epoch == self.epochs:\n",
    "                break\n",
    "                \n",
    "        #pcode:\n",
    "        #while True:\n",
    "        #    [1] data_batch = sample_training_data(data, 256) # sample 256 examples\n",
    "        #    weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n",
    "        #    weights += - step_size * weights_grad # perform parameter update        \n",
    "\n",
    "    def _minibatches(self, X: np.ndarray ,y: np.ndarray ,batchsize : int = 32, offset : int = 0):\n",
    "        ## returns a slice of training data (X and y) of specified batchsize and optional offset into the arrays\n",
    "        start_pos = 0 + offset\n",
    "        shift_index = 0\n",
    "        while start_pos < X.shape[0]:\n",
    "            block = slice(start_pos, start_pos + batchsize)\n",
    "            yield X[block], y[block]\n",
    "            if (batchsize + start_pos) > X.shape[0]:\n",
    "                shift_index += 1\n",
    "            start_pos = shift_index + ((batchsize + start_pos) % X.shape[0])\n",
    "        \n",
    "    def _unique_labels(self, y: np.ndarray):\n",
    "        \"\"\"assembles the unique labels in an array, returns count and label vector\"\"\"\n",
    "        labels = np.unique(y)\n",
    "        return (labels).shape[0], labels\n",
    "\n",
    "    def _initialize_weights(self, X:np.ndarray, y:np.ndarray):\n",
    "        \"\"\"initializes a weight vector for each label \"\"\"\n",
    "        label_count, labels = self._unique_labels(y)\n",
    "        ## initializing with random numbers\n",
    "        w = np.random.rand(label_count,X.shape[1])\n",
    "        self.w = w\n",
    "\n",
    "    def l2_regularization_euclidean(self, w : np.ndarray ,reg_const=.01):\n",
    "        return reg_const * np.linalg.norm(w) \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        print('pred')\n",
    "        pred = None\n",
    "        \"\"\"\n",
    "        Use the trained weights of svm classifier to predict labels for\n",
    "        data points.\n",
    "\n",
    "        Inputs:\n",
    "        - X_test: A numpy array of shape (N, D) containing training data; there are N\n",
    "          training samples each of dimension D.\n",
    "\n",
    "        Returns:\n",
    "        - pred: Predicted labels for the data in X_test. pred is a 1-dimensional\n",
    "          array of length N, and each element is an integer giving the predicted\n",
    "          class.\n",
    "        \"\"\"\n",
    "        if self.debug: print('w', self.w.shape, 'xT',np.transpose(X_test).shape,  X_test.shape)\n",
    "        \n",
    "        pred = X_test.dot(np.transpose(self.w))\n",
    "        \n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (with SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-12 16:08:28.629205\n",
      "-134.36308163265306 134.95661224489794 -17.242317429315477\n",
      "y_train (49000,)\n",
      "my 322237.40485397715\n",
      "loss at 1th 5962.889788532857\n",
      "my 34254.22556773161\n",
      "loss at 51th 69059.17648930868\n",
      "my 169963.29476600798\n",
      "loss at 101th 167988.3763674995\n",
      "my 1075749.2774581518\n",
      "loss at 151th 241454.85666675388\n"
     ]
    }
   ],
   "source": [
    "\n",
    "svm = SVM(\n",
    "    alpha=.01\n",
    "    , epochs=200\n",
    "    , learning_rate = .0001\n",
    "    , reg_const= .05\n",
    "    , minibatch_sz = 256)\n",
    "\n",
    "from datetime import datetime \n",
    "print(datetime.now())\n",
    "svm.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204.30259751852182\n"
     ]
    }
   ],
   "source": [
    "print(20599218/100827)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will implement a \"soft margin\" SVM. In this formulation you will maximize the margin between positive and negative training examples and penalize margin violations using a hinge loss.\n",
    "\n",
    "We will optimize the SVM loss using SGD. This means you must compute the loss function with respect to model weights. You will use this gradient to update the model weights.\n",
    "\n",
    "SVM optimized with SGD has 3 hyperparameters that you can experiment with :\n",
    "- **Learning rate** - similar to as defined above in Perceptron, this parameter scales by how much the weights are changed according to the calculated gradient update. \n",
    "- **Epochs** - similar to as defined above in Perceptron.\n",
    "- **Regularization constant** - Hyperparameter to determine the strength of regularization. In this case it is a coefficient on the term which maximizes the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will implement the SVM using SGD in the **models/SVM.py**\n",
    "\n",
    "The following code: \n",
    "- Creates an instance of the SVM classifier class \n",
    "- The train function of the SVM class is trained on the training data\n",
    "- We use the predict function to find the training accuracy as well as the testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm = svm.predict(X_train)\n",
    "print('The training accuracy is given by : %f' % (get_acc(pred_svm, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm = svm.predict(X_val)\n",
    "print('The validation accuracy is given by : %f' % (get_acc(pred_svm, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm = svm.predict(X_test)\n",
    "print('The testing accuracy is given by : %f' % (get_acc(pred_svm, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Kaggle Submission\n",
    "\n",
    "Once you are satisfied with your solution and test accuracy output a file to submit your test set predictions to the Kaggle for Assignment 1 SVM. Use the following code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submission_csv('svm_submission.csv', svm.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier (with SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, you will train a Softmax classifier. This classifier consists of a linear function of the input data followed by a softmax function which outputs a vector of dimension C (number of classes) for each data point. Each entry of the softmax output vector corresponds to a confidence in one of the C classes, and like a probability distribution, the entries of the output vector sum to 1. We use a cross-entropy loss on this sotmax output to train the model. \n",
    "\n",
    "Check the following link as an additional resource on softmax classification: http://cs231n.github.io/linear-classify/#softmax\n",
    "\n",
    "Once again we will train the classifier with SGD. This means you need to compute the gradients of the softmax cross-entropy loss function according to the weights and update the weights using this gradient. Check the following link to help with implementing the gradient updates: https://deepnotes.io/softmax-crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax classifier has 3 hyperparameters that you can experiment with :\n",
    "- **Learning rate** - As above, this controls how much the model weights are updated with respect to their gradient.\n",
    "- **Number of Epochs** - As described for perceptron.\n",
    "- **Regularization constant** - Hyperparameter to determine the strength of regularization. In this case, we minimize the L2 norm of the model weights as regularization, so the regularization constant is a coefficient on the L2 norm in the combined cross-entropy and regularization objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will implement a softmax classifier using SGD in the **models/Softmax.py**\n",
    "\n",
    "The following code: \n",
    "- Creates an instance of the Softmax classifier class \n",
    "- The train function of the Softmax class is trained on the training data\n",
    "- We use the predict function to find the training accuracy as well as the testing accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = Softmax()\n",
    "softmax.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_softmax = softmax.predict(X_train)\n",
    "print('The training accuracy is given by : %f' % (get_acc(pred_softmax, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_softmax = softmax.predict(X_val)\n",
    "print('The validation accuracy is given by : %f' % (get_acc(pred_softmax, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_softmax = softmax.predict(X_test)\n",
    "print('The testing accuracy is given by : %f' % (get_acc(pred_softmax, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Kaggle Submission\n",
    "\n",
    "Once you are satisfied with your solution and test accuracy output a file to submit your test set predictions to the Kaggle for Assignment 1 Softmax. Use the following code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submission_csv('softmax_submission.csv', softmax.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
