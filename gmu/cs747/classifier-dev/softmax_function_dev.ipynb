{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3721.146406415495\n",
      "3762.223712535063\n",
      "3805.0654809809957\n",
      "3849.8149360529237\n",
      "3896.6257758518727\n",
      "3945.6629789774984\n",
      "3997.103549120447\n",
      "4051.137372853472\n",
      "4107.96833082464\n",
      "4167.815433464159\n",
      "4230.913164374006\n",
      "4297.509832112978\n",
      "4367.863028050546\n",
      "4442.232357919447\n",
      "4520.870817762589\n",
      "4604.016792881642\n",
      "4691.889570513972\n",
      "4784.693133720901\n",
      "4882.631131804614\n",
      "4985.924799405464\n",
      "5094.814117404468\n",
      "5209.53291372533\n",
      "5330.278293976851\n",
      "5457.198989482845\n",
      "5590.395377414179\n",
      "5729.915189555786\n",
      "5875.7462875781775\n",
      "6027.794983232397\n",
      "6185.820042419749\n",
      "6349.338282807265\n",
      "6517.62208220619\n",
      "6689.867052291019\n",
      "6865.38761230442\n",
      "7043.625890952972\n",
      "7224.009870016514\n",
      "7405.929099816329\n",
      "7588.898180497995\n",
      "7772.697792598056\n",
      "7957.356783274406\n",
      "8142.989283789383\n",
      "8329.584971362325\n",
      "8516.93727873815\n",
      "8704.764110539345\n",
      "8892.842937757176\n",
      "9081.04292228328\n",
      "9269.29824748422\n",
      "9457.578245015427\n",
      "9645.869122143651\n",
      "9834.16477355684\n",
      "10022.46251563392\n",
      "10210.761172380948\n",
      "10399.060229145463\n",
      "10587.359460825779\n",
      "10775.65876898825\n",
      "10963.958110592881\n",
      "11152.2574668209\n",
      "11340.556829443925\n",
      "11528.8561948641\n",
      "11717.155561508196\n",
      "11905.454928688312\n",
      "12093.754296103605\n",
      "12282.053663622548\n",
      "12470.35303118759\n",
      "12658.652398773585\n",
      "12846.951766369537\n",
      "13035.251133970618\n",
      "13223.550501574735\n",
      "13411.849869180965\n",
      "13600.149236788895\n",
      "13788.448604398363\n",
      "13976.747972009282\n",
      "14165.047339621628\n",
      "14353.34670723537\n",
      "14541.646074850512\n",
      "14729.945442467055\n",
      "14918.244810084985\n",
      "15106.544177704312\n",
      "15294.843545325033\n",
      "15483.142912947142\n",
      "15671.442280570644\n",
      "15859.741648195548\n",
      "16048.041015821835\n",
      "16236.34038344951\n",
      "16424.639751078572\n",
      "16612.93911870902\n",
      "16801.238486340873\n",
      "16989.537853974107\n",
      "17177.83722160873\n",
      "17366.136589244743\n",
      "17554.435956882135\n",
      "17742.735324520927\n",
      "17931.034692161105\n",
      "18119.334059802666\n",
      "18307.633427445617\n",
      "18495.93279508995\n",
      "18684.23216273568\n",
      "18872.531530382792\n",
      "19060.830898031283\n",
      "19249.130265681164\n",
      "19437.42963333242\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def softmax(X:np.ndarray,y:np.ndarray,W:np.ndarray,b:np.ndarray,reg_const:float) -> (np.ndarray, float):\n",
    "    #apply the model to the training example\n",
    "    scores = X.dot(W.T)\n",
    "    log_K = -np.max(scores)\n",
    "    scores += log_K ## ensures numerical stability\n",
    "\n",
    "    if False:\n",
    "        print(\"scores\", scores)\n",
    "        print(\"b\", b)\n",
    "        print(\"scores\", scores.shape, \"b\", b.shape, \"b.T\", b.T.shape)\n",
    "    \n",
    "    #add the bias terms\n",
    "    scores += b\n",
    "    if False: print(\"+ b\", scores)\n",
    "    ##print(\"scores_0\", np.sum(scores[0]))    \n",
    "    \n",
    "    # exponentiate the scores\n",
    "    scores = np.exp(scores)\n",
    "    ##print(\"scores_0\", np.sum(scores[0]))    \n",
    "    if False:\n",
    "        print(\"exp scores\",scores)\n",
    "        print(scores.dot(np.ones(3))) ##reassurance that the sums is accurate\n",
    "    \n",
    "    # isolate the scores for the true class, y_i, for each training example\n",
    "    y_i_scores = scores[np.arange(scores.shape[0]),y]\n",
    "    \n",
    "    # calculate the base (sum of exponentiataed scores) for each training example\n",
    "    sums = np.sum(scores,axis=1).reshape(-1,1)\n",
    "\n",
    "    if False:\n",
    "        print(\"-\" * 10 +\"proving that the math is correct\" + \"-\"*20)\n",
    "        print(scores[0], \" divided by base\", sums[0])\n",
    "        print(\"produces values\", (scores[0]/sums[0]),\"which add to \", (scores[0]/sums[0]).dot(np.ones(3)))\n",
    "    \n",
    "    true_class = y_i_scores / sums\n",
    "    ##print(np.sum(true_class))\n",
    "    if False:\n",
    "        print(\"true_class\" , true_class)\n",
    "        print(\"total loss for batch\", np.sum(-np.log(true_class)), \"samples:\",  X.shape[0])\n",
    "    mean_loss_for_batch = (np.sum(-np.log(true_class)) / X.shape[0]) + (reg_const * np.linalg.norm(W))/X.shape[0]\n",
    "    \n",
    "    ## gradient \n",
    "    grad_w = np.zeros(W.shape)\n",
    "    \n",
    "    inv_sums = (1/sums).reshape(-1,1)\n",
    "    es=scores*inv_sums\n",
    "    \n",
    "    ## Gradient w.r.t w_j, all j\n",
    "    grad_w=es.T.dot(X)\n",
    "    \n",
    "    ## Gradient w.r.t w_y_i: adjust value by -x value where j==y_i (true class)\n",
    "    grad_w[y] -= X[y]\n",
    "    \n",
    "    mean_grad_w = grad_w / X.shape[0]\n",
    "    return mean_grad_w, mean_loss_for_batch\n",
    "    \n",
    "\n",
    "### tests\n",
    "W=np.array([[0.01, -0.05, 0.1, 0.05]\n",
    ", [0.7, 0.2, 0.05, 0.16]\n",
    ", [0.0, -0.45, -0.02, 0.03]])\n",
    "X=np.array([[-15,22,-44,56], [-5,10,32,43]])\n",
    "y=np.array([[1,1]])\n",
    "b=np.array([[0.0,0.2,-0.3]])\n",
    "dorandom=True\n",
    "N=256\n",
    "C=10\n",
    "Y=10\n",
    "M=3300\n",
    "if dorandom:\n",
    "    X = np.random.rand(N,M) # data\n",
    "    W = np.random.rand(C,M) ## weights\n",
    "    y = np.random.randint(low=0, high=C, size=(1,N))\n",
    "    b = np.random.rand(1,C) * 10\n",
    "    reg_const = 0.05\n",
    "    alpha = .001\n",
    "    learning_rate = .001\n",
    "    epochs = 100\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        gradient, loss = softmax(X,y,W,b,reg_const)\n",
    "        W += alpha*gradient\n",
    "        print(loss)\n",
    "        epoch += 1\n",
    "        if epoch == epochs: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.0168927  0.44378625 0.93405748 ... 0.74085568 0.38352267 0.64221418] [nan nan nan ... nan nan nan] nan\n",
      "1 [0.0168927  0.44378625 0.93405748 ... 0.74085568 0.38352267 0.64221418] [nan nan nan ... nan nan nan] nan\n",
      "2 [0.0168927  0.44378625 0.93405748 ... 0.74085568 0.38352267 0.64221418] [nan nan nan ... nan nan nan] nan\n",
      "3 [0.0168927  0.44378625 0.93405748 ... 0.74085568 0.38352267 0.64221418] [nan nan nan ... nan nan nan] nan\n",
      "4 [0.0168927  0.44378625 0.93405748 ... 0.74085568 0.38352267 0.64221418] [nan nan nan ... nan nan nan] nan\n",
      "5 [0.0168927  0.44378625 0.93405748 ... 0.74085568 0.38352267 0.64221418] [nan nan nan ... nan nan nan] nan\n",
      "6 [0.0168927  0.44378625 0.93405748 ... 0.74085568 0.38352267 0.64221418] [nan nan nan ... nan nan nan] nan\n",
      "7 [0.0168927  0.44378625 0.93405748 ... 0.74085568 0.38352267 0.64221418] [nan nan nan ... nan nan nan] nan\n",
      "8 [0.0168927  0.44378625 0.93405748 ... 0.74085568 0.38352267 0.64221418] [nan nan nan ... nan nan nan] nan\n",
      "9 [0.0168927  0.44378625 0.93405748 ... 0.74085568 0.38352267 0.64221418] [nan nan nan ... nan nan nan] nan\n"
     ]
    }
   ],
   "source": [
    "np.array([0.0, -0.45, -0.02, 0.03]).dot(np.array([-15,22,-44,56]))\n",
    "for i in range(W.shape[0]):\n",
    "    #print(W[i])\n",
    "    #print()\n",
    "    print(i,X[0],W[i], W[i].dot(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.21],\n",
       "       [1.1 ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _softmax( escore):\n",
    "    tmp = escore - escore.max(axis=1).reshape(-1, 1)\n",
    "    exp_tmp = np.exp(tmp)\n",
    "    return exp_tmp / exp_tmp.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "scores =np.array( [[.5,.7,.01],[.3,.8,.0]] )\n",
    "_softmax(scores)\n",
    "\n",
    "scores.sum(axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6881171418161356e+43"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.718281828459045\n",
      "10 22026.465794806718\n",
      "100 2.6881171418161356e+43\n",
      "1000 inf\n",
      "10000 inf\n",
      "100000 inf\n",
      "1000000 inf\n",
      "10000000 inf\n",
      "100000000 inf\n",
      "1000000000 inf\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(10**i, np.exp(10**i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([501, 502, 503, 504])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([1,2,3,4])\n",
    "a + 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
